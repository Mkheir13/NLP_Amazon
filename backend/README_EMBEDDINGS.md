# üîó Module d'Embeddings - NLP Amazon Analysis

## Vue d'ensemble

Le module d'embeddings ajoute des capacit√©s avanc√©es de repr√©sentation vectorielle et de recherche s√©mantique √† votre projet NLP Amazon Analysis. Il permet de :

- **Entra√Æner des mod√®les Word2Vec** personnalis√©s sur vos donn√©es
- **Visualiser les embeddings** en 2D/3D avec t-SNE, PCA et UMAP
- **Effectuer des recherches s√©mantiques** dans des collections de textes
- **Analyser les relations s√©mantiques** entre les mots

## üöÄ Installation

### D√©pendances requises

```bash
pip install gensim>=4.3.0
pip install sentence-transformers>=2.2.0
pip install plotly>=5.17.0
pip install umap-learn>=0.5.4
pip install matplotlib>=3.7.0
pip install seaborn>=0.12.0
```

### Installation automatique
Les d√©pendances sont list√©es dans `requirements.txt` et s'installent avec :
```bash
pip install -r requirements.txt
```

## üìö API Endpoints

### Entra√Ænement de mod√®les

#### `POST /api/embeddings/train/word2vec`
Entra√Æne un nouveau mod√®le Word2Vec.

**Param√®tres :**
```json
{
  "texts": ["liste", "de", "textes"],
  "config": {
    "vector_size": 100,
    "window": 5,
    "min_count": 2,
    "workers": 4,
    "epochs": 10,
    "sg": 1
  }
}
```

**R√©ponse :**
```json
{
  "success": true,
  "model": {
    "id": "20241220_143022",
    "type": "word2vec",
    "vocabulary_size": 1500,
    "trained_on": 1000,
    "config": {...}
  }
}
```

### R√©cup√©ration d'embeddings

#### `GET /api/embeddings/word/{word}`
Obtient l'embedding d'un mot sp√©cifique.

**Param√®tres :**
- `model_id` (optionnel) : ID du mod√®le √† utiliser

**R√©ponse :**
```json
{
  "success": true,
  "word": "good",
  "embedding": [0.1, -0.2, 0.3, ...],
  "dimension": 100
}
```

#### `POST /api/embeddings/sentence`
Obtient l'embedding d'une phrase avec Sentence-BERT.

**Param√®tres :**
```json
{
  "text": "This is a sample sentence"
}
```

### Recherche de similarit√©

#### `GET /api/embeddings/similar/{word}`
Trouve les mots les plus similaires √† un mot donn√©.

**Param√®tres :**
- `top_k` : nombre de r√©sultats (d√©faut: 10)
- `model_id` (optionnel) : ID du mod√®le

**R√©ponse :**
```json
{
  "success": true,
  "word": "good",
  "similar_words": [
    ["great", 0.85],
    ["excellent", 0.82],
    ["nice", 0.78]
  ]
}
```

### Recherche s√©mantique

#### `POST /api/embeddings/search`
Effectue une recherche s√©mantique dans une collection de textes.

**Param√®tres :**
```json
{
  "query": "good product quality",
  "texts": ["list of texts to search"],
  "top_k": 5
}
```

**R√©ponse :**
```json
{
  "success": true,
  "results": [
    {
      "index": 0,
      "text": "This product has excellent quality",
      "similarity": 0.89,
      "text_preview": "This product has excellent..."
    }
  ]
}
```

### Visualisation

#### `POST /api/embeddings/visualize`
G√©n√®re une visualisation 2D des embeddings.

**Param√®tres :**
```json
{
  "words": ["good", "bad", "excellent", "terrible"],
  "method": "tsne",
  "model_id": "20241220_143022"
}
```

**M√©thodes disponibles :**
- `pca` : Analyse en Composantes Principales
- `tsne` : t-SNE (t-Distributed Stochastic Neighbor Embedding)
- `umap` : UMAP (Uniform Manifold Approximation and Projection)

### Gestion des mod√®les

#### `GET /api/embeddings/models`
Liste tous les mod√®les d'embedding disponibles.

#### `GET /api/embeddings/stats`
Obtient les statistiques d'un mod√®le.

**Param√®tres :**
- `model_id` (optionnel) : ID du mod√®le

## üéØ Utilisation Frontend

### Composants disponibles

1. **EmbeddingTraining** : Interface pour entra√Æner des mod√®les Word2Vec
2. **EmbeddingVisualizer** : Visualisation interactive des embeddings
3. **SemanticSearch** : Interface de recherche s√©mantique

### Exemple d'utilisation

```tsx
import { EmbeddingService } from '../services/EmbeddingService';

// Entra√Æner un mod√®le
const model = await EmbeddingService.trainWord2Vec(texts, {
  vector_size: 100,
  window: 5,
  epochs: 10
});

// Recherche s√©mantique
const results = await EmbeddingService.semanticSearch(
  "good quality product",
  reviewTexts,
  10
);

// Visualiser des embeddings
const visualization = await EmbeddingService.visualizeEmbeddings(
  ["good", "bad", "excellent"],
  "tsne",
  modelId
);
```

## ‚öôÔ∏è Configuration

### Param√®tres Word2Vec

- **vector_size** : Taille des vecteurs d'embedding (50-300)
- **window** : Taille de la fen√™tre contextuelle (3-10)
- **min_count** : Fr√©quence minimale des mots (1-10)
- **workers** : Nombre de threads parall√®les (1-8)
- **epochs** : Nombre d'√©poques d'entra√Ænement (5-20)
- **sg** : Algorithme (1=Skip-gram, 0=CBOW)

### Recommandations

- **Petits datasets** (<1000 textes) : vector_size=50, window=3, epochs=15
- **Datasets moyens** (1000-10000) : vector_size=100, window=5, epochs=10
- **Gros datasets** (>10000) : vector_size=200, window=7, epochs=5

## üîß Architecture technique

### Backend (Python)
- **EmbeddingService** : Service principal pour la gestion des embeddings
- **Gensim** : Entra√Ænement des mod√®les Word2Vec
- **Sentence-Transformers** : Embeddings de phrases pr√©-entra√Æn√©s
- **Plotly** : G√©n√©ration des visualisations interactives
- **UMAP/t-SNE** : R√©duction de dimensionnalit√©

### Frontend (React/TypeScript)
- **EmbeddingService.ts** : Client API pour les embeddings
- **Composants React** : Interfaces utilisateur interactives
- **Plotly.js** : Rendu des visualisations c√¥t√© client

## üé® Fonctionnalit√©s avanc√©es

### 1. Analyse s√©mantique compl√®te
```python
analysis = embedding_service.analyze_text_semantics(text)
# Retourne : embedding de phrase, similarit√©s entre mots, densit√© s√©mantique
```

### 2. Visualisation interactive
- Zoom et pan sur les graphiques
- Hover pour voir les d√©tails des mots
- Export des visualisations
- Comparaison de diff√©rentes m√©thodes

### 3. Recherche s√©mantique avanc√©e
- Recherche dans le dataset Amazon (500+ avis)
- Recherche dans des textes personnalis√©s
- Tri par score de similarit√©
- Historique des recherches

## üö® D√©pannage

### Erreurs communes

1. **"Service d'embedding non disponible"**
   - V√©rifiez que les d√©pendances sont install√©es
   - Red√©marrez le backend
   - V√©rifiez les logs pour les erreurs d'import

2. **"Aucun mod√®le Word2Vec disponible"**
   - Entra√Ænez d'abord un mod√®le via l'interface
   - V√©rifiez que le dossier `models/embeddings` existe

3. **"Mot non trouv√© dans le vocabulaire"**
   - Le mot n'√©tait pas assez fr√©quent lors de l'entra√Ænement
   - R√©duisez le param√®tre `min_count`
   - Entra√Ænez avec plus de donn√©es

### Optimisation des performances

1. **Entra√Ænement lent**
   - Augmentez le nombre de `workers`
   - R√©duisez `vector_size` ou `epochs`
   - Utilisez CBOW au lieu de Skip-gram

2. **Visualisation lente**
   - Limitez le nombre de mots (<50)
   - Utilisez PCA au lieu de t-SNE pour de gros datasets
   - R√©duisez la taille des embeddings

## üìà Exemples d'utilisation

### Analyse de sentiment avec embeddings
```python
# Entra√Æner sur les avis Amazon
model = train_word2vec(amazon_reviews)

# Analyser les relations s√©mantiques
similar_to_good = model.wv.most_similar('good', topn=10)
# ['great', 'excellent', 'amazing', 'wonderful', ...]

# Recherche s√©mantique
results = semantic_search("poor quality", reviews, top_k=5)
# Trouve les avis parlant de mauvaise qualit√© m√™me sans les mots exacts
```

### Exploration de domaine
```python
# Visualiser les mots d'un domaine
tech_words = ['phone', 'battery', 'screen', 'camera', 'performance']
visualization = visualize_embeddings(tech_words, method='tsne')
# Voir comment ces concepts se regroupent dans l'espace s√©mantique
```

## üîÆ D√©veloppements futurs

- **Embeddings contextuels** : Int√©gration de BERT/RoBERTa
- **Embeddings multilingues** : Support de plusieurs langues
- **Clustering automatique** : D√©tection de groupes s√©mantiques
- **Comparaison de mod√®les** : Interface pour comparer diff√©rents embeddings
- **Export/Import** : Sauvegarde et partage de mod√®les

## üìû Support

Pour toute question ou probl√®me :
1. Consultez les logs du backend
2. V√©rifiez la documentation des d√©pendances
3. Ouvrez une issue sur GitHub avec les d√©tails de l'erreur 