#!/usr/bin/env python3
"""
Dataset Amazon Polarity - TÃ©lÃ©chargement et chargement du dataset complet
TÃ©lÃ©charge le vrai dataset Amazon Polarity (3.6M train + 400K test)
"""

import os
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def download_amazon_polarity_dataset(force_download: bool = False) -> bool:
    """
    TÃ©lÃ©charge le dataset Amazon Polarity complet depuis Hugging Face
    
    Args:
        force_download: Force le re-tÃ©lÃ©chargement mÃªme si les fichiers existent
    
    Returns:
        bool: True si le tÃ©lÃ©chargement rÃ©ussit, False sinon
    """
    try:
        from datasets import load_dataset
        
        # Chemins des fichiers
        train_path = "data/amazon_polarity/train.csv"
        test_path = "data/amazon_polarity/test.csv"
        
        # VÃ©rifier si les fichiers existent dÃ©jÃ  et sont volumineux
        if not force_download:
            if (os.path.exists(train_path) and os.path.exists(test_path)):
                train_size = os.path.getsize(train_path) / (1024 * 1024)  # MB
                test_size = os.path.getsize(test_path) / (1024 * 1024)   # MB
                
                # Si les fichiers sont volumineux (dataset complet), pas besoin de re-tÃ©lÃ©charger  
                if train_size > 100 and test_size > 20:  # Au moins 100MB train et 20MB test
                    logger.info(f"âœ… Dataset complet dÃ©jÃ  prÃ©sent (Train: {train_size:.1f}MB, Test: {test_size:.1f}MB)")
                    return True
        
        logger.info("ðŸ“¥ TÃ©lÃ©chargement du dataset Amazon Polarity complet...")
        logger.info("âš ï¸  ATTENTION: Le dataset complet fait ~500MB, cela peut prendre plusieurs minutes")
        
        # CrÃ©er le rÃ©pertoire si nÃ©cessaire
        os.makedirs("data/amazon_polarity", exist_ok=True)
        
        # TÃ©lÃ©charger le dataset complet
        dataset = load_dataset("amazon_polarity")
        
        # Convertir en DataFrames
        train_df = pd.DataFrame(dataset['train'])
        test_df = pd.DataFrame(dataset['test'])
        
        # Mapper les labels (0,1) vers (1,2) pour correspondre au format attendu
        train_df['label'] = train_df['label'] + 1
        test_df['label'] = test_df['label'] + 1
        
        # Renommer les colonnes pour correspondre au format attendu
        train_df = train_df.rename(columns={'content': 'text'})
        test_df = test_df.rename(columns={'content': 'text'})
        
        # Sauvegarder les fichiers CSV
        train_df.to_csv(train_path, index=False)
        test_df.to_csv(test_path, index=False)
        
        logger.info(f"âœ… Dataset tÃ©lÃ©chargÃ© avec succÃ¨s!")
        logger.info(f"ðŸ“Š Train: {len(train_df):,} Ã©chantillons")
        logger.info(f"ðŸ“Š Test: {len(test_df):,} Ã©chantillons")
        logger.info(f"ðŸ’¾ Taille train: {os.path.getsize(train_path) / (1024*1024):.1f}MB")
        logger.info(f"ðŸ’¾ Taille test: {os.path.getsize(test_path) / (1024*1024):.1f}MB")
        
        return True
        
    except ImportError:
        logger.error("âŒ La librairie 'datasets' n'est pas installÃ©e")
        logger.error("ðŸ’¡ Installez avec: pip install datasets")
        return False
    except Exception as e:
        logger.error(f"âŒ Erreur lors du tÃ©lÃ©chargement: {e}")
        return False

def load_amazon_polarity_dataset(
    max_samples: int = None,
    split: str = "train",
    random_sample: bool = False,
    use_full_dataset: bool = True
) -> List[Dict]:
    """
    Charge le dataset Amazon Polarity
    
    Args:
        max_samples: Nombre maximum d'Ã©chantillons Ã  charger (None = tous)
        split: 'train', 'test', ou 'all'
        random_sample: Si True, Ã©chantillonnage alÃ©atoire
        use_full_dataset: Si True, tente de tÃ©lÃ©charger le dataset complet
    
    Returns:
        List[Dict]: Liste des Ã©chantillons avec 'text' et 'label'
    """
    
    # Tentative de tÃ©lÃ©chargement du dataset complet
    if use_full_dataset:
        download_success = download_amazon_polarity_dataset()
        if not download_success:
            logger.warning("âš ï¸  Ã‰chec du tÃ©lÃ©chargement, utilisation du dataset de fallback")
            return load_fallback_dataset(max_samples, split, random_sample)
    
    try:
        # Chemins des fichiers
        train_path = "data/amazon_polarity/train.csv"
        test_path = "data/amazon_polarity/test.csv"
        
        # Charger les donnÃ©es selon le split demandÃ©
        if split == "train":
            if not os.path.exists(train_path):
                logger.warning("âŒ Fichier train.csv non trouvÃ©, utilisation du fallback")
                return load_fallback_dataset(max_samples, split, random_sample)
            df = pd.read_csv(train_path)
        elif split == "test":
            if not os.path.exists(test_path):
                logger.warning("âŒ Fichier test.csv non trouvÃ©, utilisation du fallback")
                return load_fallback_dataset(max_samples, split, random_sample)
            df = pd.read_csv(test_path)
        elif split == "all":
            dfs = []
            if os.path.exists(train_path):
                dfs.append(pd.read_csv(train_path))
            if os.path.exists(test_path):
                dfs.append(pd.read_csv(test_path))
            if not dfs:
                logger.warning("âŒ Aucun fichier trouvÃ©, utilisation du fallback")
                return load_fallback_dataset(max_samples, split, random_sample)
            df = pd.concat(dfs, ignore_index=True)
        else:
            logger.error(f"âŒ Split invalide: {split}")
            return []
        
        # VÃ©rifier les colonnes nÃ©cessaires
        if 'text' not in df.columns or 'label' not in df.columns:
            logger.error("âŒ Colonnes 'text' et 'label' requises")
            return load_fallback_dataset(max_samples, split, random_sample)
        
        # Nettoyer les donnÃ©es
        df = df.dropna(subset=['text', 'label'])
        
        # Ã‰chantillonnage si demandÃ©
        if max_samples and len(df) > max_samples:
            if random_sample:
                df = df.sample(n=max_samples, random_state=42)
            else:
                df = df.head(max_samples)
        
        # Convertir en format attendu
        samples = []
        for _, row in df.iterrows():
            samples.append({
                'text': str(row['text']),
                'label': int(row['label'])
            })
        
        logger.info(f"âœ… Dataset chargÃ©: {len(samples):,} Ã©chantillons ({split})")
        
        # Statistiques
        if samples:
            labels = [s['label'] for s in samples]
            pos_count = sum(1 for l in labels if l == 2)
            neg_count = sum(1 for l in labels if l == 1)
            logger.info(f"ðŸ“Š Positifs: {pos_count:,} | NÃ©gatifs: {neg_count:,}")
        
        return samples
        
    except Exception as e:
        logger.error(f"âŒ Erreur lors du chargement: {e}")
        return load_fallback_dataset(max_samples, split, random_sample)

def load_fallback_dataset(
    max_samples: int = None,
    split: str = "train", 
    random_sample: bool = False
) -> List[Dict]:
    """
    Dataset de fallback si le tÃ©lÃ©chargement Ã©choue
    """
    logger.info("ðŸ”„ Utilisation du dataset de fallback...")
    
    # DonnÃ©es de base pour les tests
    fallback_samples = [
        {"text": "This product is amazing! Great quality and fast shipping.", "label": 2},
        {"text": "Excellent purchase, highly recommend to everyone.", "label": 2},
        {"text": "Perfect item, exactly as described.", "label": 2},
        {"text": "Outstanding quality, exceeded my expectations.", "label": 2},
        {"text": "Fantastic product, will buy again.", "label": 2},
        {"text": "Terrible quality, broke after one day.", "label": 1},
        {"text": "Worst purchase ever, complete waste of money.", "label": 1},
        {"text": "Poor quality materials, very disappointed.", "label": 1},
        {"text": "Awful product, doesn't work as advertised.", "label": 1},
        {"text": "Horrible experience, would not recommend.", "label": 1},
    ]
    
    # Dupliquer pour avoir plus de donnÃ©es si nÃ©cessaire
    if max_samples and max_samples > len(fallback_samples):
        multiplier = (max_samples // len(fallback_samples)) + 1
        fallback_samples = fallback_samples * multiplier
    
    # Appliquer les filtres
    if max_samples:
        if random_sample:
            np.random.seed(42)
            indices = np.random.choice(len(fallback_samples), min(max_samples, len(fallback_samples)), replace=False)
            fallback_samples = [fallback_samples[i] for i in indices]
        else:
            fallback_samples = fallback_samples[:max_samples]
    
    logger.info(f"âœ… Dataset fallback: {len(fallback_samples)} Ã©chantillons")
    return fallback_samples

def get_dataset_info() -> Dict:
    """
    Retourne des informations sur le dataset disponible
    """
    train_path = "data/amazon_polarity/train.csv"
    test_path = "data/amazon_polarity/test.csv"
    
    info = {
        "train_exists": os.path.exists(train_path),
        "test_exists": os.path.exists(test_path),
        "train_size": 0,
        "test_size": 0,
        "train_file_size_mb": 0,
        "test_file_size_mb": 0,
        "is_full_dataset": False
    }
    
    if info["train_exists"]:
        try:
            df = pd.read_csv(train_path)
            info["train_size"] = len(df)
            info["train_file_size_mb"] = os.path.getsize(train_path) / (1024 * 1024)
        except:
            pass
    
    if info["test_exists"]:
        try:
            df = pd.read_csv(test_path)
            info["test_size"] = len(df)
            info["test_file_size_mb"] = os.path.getsize(test_path) / (1024 * 1024)
        except:
            pass
    
    # DÃ©terminer si c'est le dataset complet (heuristique basÃ©e sur la taille)
    info["is_full_dataset"] = (
        info["train_size"] > 1000000 and  # Plus d'1M d'Ã©chantillons d'entraÃ®nement
        info["test_size"] > 100000        # Plus de 100K d'Ã©chantillons de test
    )
    
    return info

class AmazonDatasetLoader:
    """
    Classe pour charger le dataset Amazon Polarity complet
    """
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def load_data(self, split: str = "train", max_samples: int = None, random_sample: bool = False) -> List[str]:
        """
        Charge les textes du dataset (format simple pour compatibilitÃ©)
        
        Returns:
            List[str]: Liste des textes seulement
        """
        samples = load_amazon_polarity_dataset(max_samples, split, random_sample, use_full_dataset=True)
        return [sample['text'] for sample in samples]
    
    def load_labeled_data(self, split: str = "train", max_samples: int = None, random_sample: bool = False) -> Tuple[List[str], List[int]]:
        """
        Charge les textes et labels du dataset
        
        Returns:
            Tuple[List[str], List[int]]: (textes, labels)
        """
        samples = load_amazon_polarity_dataset(max_samples, split, random_sample, use_full_dataset=True)
        texts = [sample['text'] for sample in samples]
        labels = [sample['label'] for sample in samples]
        return texts, labels
    
    def get_statistics(self) -> Dict:
        """
        Retourne les statistiques du dataset
        """
        return get_dataset_info()

# Instance globale pour compatibilitÃ©
amazon_loader = AmazonDatasetLoader()

if __name__ == "__main__":
    # Test du tÃ©lÃ©chargement
    print("ðŸš€ Test du tÃ©lÃ©chargement du dataset Amazon Polarity complet...")
    
    # Afficher les infos actuelles
    info = get_dataset_info()
    print(f"ðŸ“Š Ã‰tat actuel:")
    print(f"   Train: {info['train_size']:,} Ã©chantillons ({info['train_file_size_mb']:.1f}MB)")
    print(f"   Test: {info['test_size']:,} Ã©chantillons ({info['test_file_size_mb']:.1f}MB)")
    print(f"   Dataset complet: {'âœ…' if info['is_full_dataset'] else 'âŒ'}")
    
    # TÃ©lÃ©charger le dataset complet
    if not info['is_full_dataset']:
        print("\nðŸ“¥ TÃ©lÃ©chargement du dataset complet...")
        success = download_amazon_polarity_dataset(force_download=True)
        
        if success:
            # Afficher les nouvelles infos
            info = get_dataset_info()
            print(f"\nâœ… TÃ©lÃ©chargement terminÃ©!")
            print(f"ðŸ“Š Nouveau Ã©tat:")
            print(f"   Train: {info['train_size']:,} Ã©chantillons ({info['train_file_size_mb']:.1f}MB)")
            print(f"   Test: {info['test_size']:,} Ã©chantillons ({info['test_file_size_mb']:.1f}MB)")
            print(f"   Dataset complet: {'âœ…' if info['is_full_dataset'] else 'âŒ'}")
        else:
            print("âŒ Ã‰chec du tÃ©lÃ©chargement")
    else:
        print("âœ… Dataset complet dÃ©jÃ  disponible!")
    
    # Test de chargement
    print("\nðŸ§ª Test de chargement...")
    samples = load_amazon_polarity_dataset(max_samples=10, split="train")
    print(f"ðŸ“Š Ã‰chantillons chargÃ©s: {len(samples)}")
    
    if samples:
        print("\nðŸ“ Premiers Ã©chantillons:")
        for i, sample in enumerate(samples[:3]):
            label_text = "Positif" if sample['label'] == 2 else "NÃ©gatif"
            text_preview = sample['text'][:100] + "..." if len(sample['text']) > 100 else sample['text']
            print(f"   {i+1}. [{label_text}] {text_preview}") 